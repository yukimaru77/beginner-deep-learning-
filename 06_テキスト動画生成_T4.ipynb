{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4p1ysFKMbs_"
   },
   "source": "# 深層学習体験会: テキストから動画生成（LTX-Video）\n\nこのノートブックでは、**LTX-Video**モデルを使って、テキストの説明文だけから動画を生成する方法を学びます。"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBB00lC6q-DA"
   },
   "source": "## テキストから動画生成とは？\n\n**Text-to-Video**は、テキストの説明文を入力するだけで、それに基づいた動画を自動生成する技術です。画像生成（Text-to-Image）の動画版と言えます。\n\n### LTX-Videoの特徴\n- **完全なテキスト制御**: プロンプトだけで動画を生成\n- **高品質出力**: 滑らかで自然な動画\n- **柔軟な設定**: 解像度、フレーム数、スタイルを調整可能\n\n### 応用例\n- 映画の絵コンテ作成\n- 広告動画の試作\n- ストーリーボードの自動生成\n- SNS用のショート動画作成\n- 教育コンテンツの視覚化\n\n## 重要な注意事項\n- **GPU要件**: 無料のT4 GPUで実行可能ですが、解像度とフレーム数によっては制限があります\n  - デフォルト設定（832x480、73フレーム）は問題なく動作\n  - 768x512で121フレームの場合、デコード時にメモリ不足になる可能性があります\n- **より高速・高品質な生成には上位GPUを推奨**\n- **フレーム数の調整**: n フレームの動画を生成したい場合は、frames を n+1 に設定してください\n  - 例: 72フレームの動画を作りたい → frames = 73 に設定\n- **プロンプトの重要性**: 詳細で具体的なプロンプトを使用すると、より良い結果が得られます\n- **FPS**: 動画は24fpsで生成されます\n\n## このノートブックの流れ\n1. 環境の準備とモデルのダウンロード\n2. プロンプトとパラメータの設定\n3. 動画の生成と再生"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rrXFIT4fMfyJ"
   },
   "outputs": [],
   "source": "# @title 環境の準備\n# --- PyTorchのインストール ---\n# PyTorch 2.6.0とtorchvision 0.21.0をインストール（深層学習フレームワーク）\n!pip install torch==2.6.0 torchvision==0.21.0\n\n# /contentディレクトリに移動\n%cd /content\n\n# --- 設定変数 ---\n# 推論用にモデルを常にロードするかどうか\nAlways_Load_Models_for_Inference = False\n# T5XXLの16bit版を使用するかどうか（Falseの場合は8bit量子化版を使用）\nUse_t5xxl_fp16 = False\n\n# --- 依存ライブラリのインストール ---\n# torchsde: 確率微分方程式ソルバー（ノイズ除去に使用）\n# einops: テンソル操作を簡潔に記述するライブラリ\n# diffusers: 拡散モデルのライブラリ\n# accelerate: モデルの高速化とメモリ最適化\n# xformers: 高速なアテンション機構\n# av: 動画ファイルの読み書き用ライブラリ\n!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2\n!pip install av\n\n# --- ComfyUIのクローンとセットアップ ---\n# ComfyUI: 動画生成のバックエンドフレームワーク\n!git clone https://github.com/Isi-dev/ComfyUI\n%cd /content/ComfyUI\n\n# aria2（高速ダウンローダー）とffmpeg（動画処理）をインストール\n!apt -y install -qq aria2 ffmpeg\n\n# --- 必要なモデルファイルをダウンロード ---\n# LTX-Videoメインモデル（約2Bパラメータ、動画生成の中核）\n!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors -d /content/ComfyUI/models/checkpoints -o ltx-video-2b-v0.9.5.safetensors\n\n# テキストエンコーダー（T5-XXL）をダウンロード\n# 条件分岐: fp16（16bit）版またはfp8（8bit量子化）版を選択\nif Use_t5xxl_fp16:\n    # 16bit版（高精度だがメモリ消費大）\n    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp16.safetensors\nelse:\n    # 8bit量子化版（省メモリで推奨）\n    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp8_e4m3fn_scaled.safetensors\n\n# --- 必要なライブラリをインポート ---\nimport torch  # PyTorchフレームワーク\nimport numpy as np  # 数値計算用\nfrom PIL import Image  # 画像処理用\nimport gc  # ガベージコレクション（メモリ管理）\nimport sys  # システム関連の操作\nimport random  # 乱数生成\nimport os  # ファイル・ディレクトリ操作\nimport imageio  # 動画ファイルの読み書き\nfrom google.colab import files  # Google Colabのファイルアップロード機能\nfrom IPython.display import display, HTML  # Jupyter Notebookでの表示用\n\n# ComfyUIのモジュールをインポートできるようにパスを追加\nsys.path.insert(0, '/content/ComfyUI')\n\n# --- ComfyUIのモジュールをインポート ---\nfrom comfy import model_management  # モデルのメモリ管理\n\n# 基本的なノード（処理単位）をインポート\nfrom nodes import (\n    CheckpointLoaderSimple,  # モデルの読み込み\n    CLIPLoader,  # テキストエンコーダーの読み込み\n    CLIPTextEncode,  # テキストのエンコード（埋め込みベクトル化）\n    VAEDecode  # 潜在表現から画像/動画へのデコード\n)\n\n# カスタムサンプラー関連のノード\nfrom comfy_extras.nodes_custom_sampler import (\n    KSamplerSelect,  # サンプリング手法の選択\n    SamplerCustom  # カスタムサンプラーの実行\n)\n\n# LTX-Video専用のノード\nfrom comfy_extras.nodes_lt import (\n    LTXVConditioning,  # LTX-Video用の条件付け\n    LTXVScheduler,  # ノイズスケジューラー\n    EmptyLTXVLatentVideo  # 空の潜在動画（ノイズから開始）\n)\n\n# --- 各ノードのインスタンスを作成 ---\n# これらのオブジェクトを通じて各処理を実行する\ncheckpoint_loader = CheckpointLoaderSimple()  # モデル読み込み用\nclip_loader = CLIPLoader()  # テキストエンコーダー読み込み用\nclip_encode_positive = CLIPTextEncode()  # ポジティブプロンプトのエンコード用\nclip_encode_negative = CLIPTextEncode()  # ネガティブプロンプトのエンコード用\nscheduler = LTXVScheduler()  # ノイズ除去スケジュール管理用\nsampler_select = KSamplerSelect()  # サンプラー選択用\nconditioning = LTXVConditioning()  # 条件付け処理用\nempty_latent_video = EmptyLTXVLatentVideo()  # 空の潜在動画生成用\nsampler = SamplerCustom()  # サンプリング実行用\nvae_decode = VAEDecode()  # VAEデコード用\n\n\ndef clear_memory():\n    \"\"\"\n    GPU（VRAM）とCPU RAMのメモリを解放する関数\n    \n    動画生成は大量のメモリを消費するため、処理の合間に\n    不要なオブジェクトを削除してメモリを確保する必要があります。\n    \"\"\"\n    # Pythonのガベージコレクションを実行（不要なオブジェクトを削除）\n    gc.collect()\n    \n    # CUDAが利用可能な場合（GPUがある場合）\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()  # GPUキャッシュをクリア\n        torch.cuda.ipc_collect()  # プロセス間通信メモリをクリア\n    \n    # グローバル変数に残っているテンソルを削除\n    for obj in list(globals().values()):\n        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n            del obj\n    \n    # 再度ガベージコレクションを実行\n    gc.collect()\n\n\ndef generate_video(\n    positive_prompt: str = \"A drone quickly rises through a bank of morning fog...\",  # 生成したい動画の説明（ポジティブプロンプト）\n    negative_prompt: str = \"low quality, worst quality...\",  # 避けたい要素（ネガティブプロンプト）\n    width: int = 768,  # 動画の幅（ピクセル、32の倍数である必要）\n    height: int = 512,  # 動画の高さ（ピクセル、32の倍数である必要）\n    seed: int = 0,  # ランダムシード（0の場合はランダム、それ以外は再現可能）\n    steps: int = 30,  # 生成ステップ数（多いほど高品質だが時間がかかる）\n    cfg_scale: float = 2.05,  # CFGスケール（プロンプトへの忠実度、大きいほど忠実）\n    sampler_name: str = \"res_multistep\",  # サンプリング手法の名前\n    length: int = 49,  # 生成するフレーム数（n+1フレームでn秒の動画）\n    fps: int = 24  # フレームレート（1秒あたりのフレーム数）\n):\n    \"\"\"\n    テキストプロンプトから動画を生成するメイン関数\n    \n    引数:\n        positive_prompt: 生成したい動画の内容を詳細に記述したテキスト\n        negative_prompt: 避けたい要素を記述したテキスト\n        width: 動画の幅（32の倍数）\n        height: 動画の高さ（32の倍数）\n        seed: ランダムシード（0=ランダム、それ以外=再現可能）\n        steps: ノイズ除去のステップ数\n        cfg_scale: Classifier-Free Guidanceのスケール（プロンプトへの忠実度）\n        sampler_name: 使用するサンプリングアルゴリズム名\n        length: 生成するフレーム数\n        fps: 動画のフレームレート（通常24fps）\n    \"\"\"\n    \n    # 推論モード（学習しない）で実行\n    with torch.inference_mode():\n        print(\"テキストエンコーダーを読み込み中...\")\n        # CLIPテキストエンコーダーを読み込む\n        # テキストプロンプトを数値ベクトルに変換するために使用\n        clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n        print(\"テキストエンコーダーを読み込みました！\")\n\n    try:\n        # --- 入力の検証 ---\n        # 幅と高さが32の倍数であることを確認（モデルの制約）\n        assert width % 32 == 0, \"幅は32の倍数である必要があります\"\n        assert height % 32 == 0, \"高さは32の倍数である必要があります\"\n\n        # --- プロンプトのエンコード ---\n        # ポジティブプロンプト（生成したい内容）を埋め込みベクトルに変換\n        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n        # ネガティブプロンプト（避けたい内容）を埋め込みベクトルに変換\n        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n\n        # テキストエンコーダーはもう不要なのでメモリから削除\n        del clip\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"テキストエンコーダーをメモリから削除しました\")\n\n        # --- 空の潜在動画を生成 ---\n        # ランダムノイズから開始するための初期状態を作成\n        empty_latent = empty_latent_video.generate(width, height, length)[0]\n\n        # --- ノイズ除去スケジュールを設定 ---\n        # sigmas: ノイズ除去の各ステップでのノイズレベルを定義\n        # 引数: (ステップ数, CFGスケール, denoise強度, シフト有効, シフトベース)\n        sigmas = scheduler.get_sigmas(steps, cfg_scale, 0.95, True, 0.1)[0]\n        \n        # サンプリング手法を選択（res_multistep, euler, dpmpp_2mなど）\n        selected_sampler = sampler_select.get_sampler(sampler_name)[0]\n        \n        # 条件付けを結合（ポジティブとネガティブのプロンプトを統合）\n        # 25.0: 動画の長さに関する条件付け強度\n        conditioned = conditioning.append(positive, negative, 25.0)\n\n        # --- モデルとVAEの読み込み ---\n        print(\"モデルとVAEを読み込み中...\")\n        # LTX-Videoのメインモデルを読み込む\n        # model: ノイズ除去モデル（潜在空間で動作）\n        # vae: 変分オートエンコーダー（潜在空間⇔ピクセル空間の変換）\n        model, _, vae = checkpoint_loader.load_checkpoint(\"ltx-video-2b-v0.9.5.safetensors\")\n        print(\"モデルとVAEを読み込みました！\")\n\n        # --- 動画の生成（サンプリング） ---\n        print(\"動画を生成中...\")\n        # ノイズ除去プロセスを実行して潜在表現を生成\n        sampled = sampler.sample(\n            model=model,  # 使用するモデル\n            add_noise=True,  # ノイズを追加するか\n            noise_seed=seed if seed != 0 else random.randint(0, 2**32),  # シード値（0の場合はランダム）\n            cfg=cfg_scale,  # CFGスケール（プロンプトへの忠実度）\n            positive=conditioned[0],  # ポジティブ条件\n            negative=conditioned[1],  # ネガティブ条件\n            sampler=selected_sampler,  # サンプリングアルゴリズム\n            sigmas=sigmas,  # ノイズ除去スケジュール\n            latent_image=empty_latent  # 初期潜在表現（ノイズ）\n        )[0]\n\n        # モデルはもう不要なのでメモリから削除\n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"モデルをメモリから削除しました\")\n\n        # --- VAEデコード（潜在表現→ピクセル） ---\n        # 勾配計算なしで実行（推論のみ）\n        with torch.no_grad():\n            try:\n                print(\"潜在表現をデコード中...\")\n                # VAEで潜在表現を実際の画像フレームにデコード\n                # sampled: 潜在空間での動画表現 → decoded: ピクセル空間での動画フレーム\n                decoded = vae_decode.decode(vae, sampled)[0].detach()\n                print(\"潜在表現をデコードしました！\")\n                \n                # VAEはもう不要なのでメモリから削除\n                del vae\n                torch.cuda.empty_cache()\n                gc.collect()\n                print(\"VAEをメモリから削除しました\")\n\n                # --- MP4ファイルとして保存 ---\n                output_path = \"/content/output.mp4\"\n                # imageioのライターで動画ファイルを作成\n                with imageio.get_writer(output_path, fps=fps) as writer:\n                    # 各フレームを処理して書き込み\n                    for i, frame in enumerate(decoded):\n                        # フレームをCPUに転送してNumPy配列に変換\n                        # [0-1]の範囲の浮動小数点数を[0-255]の整数に変換\n                        frame_np = (frame.cpu().numpy() * 255).astype(np.uint8)\n                        # フレームを動画ファイルに追加\n                        writer.append_data(frame_np)\n                        # 定期的にメモリをクリア（10フレームごと）\n                        if i % 10 == 0:\n                            torch.cuda.empty_cache()\n\n                print(f\"正常に{len(decoded)}フレームを処理しました\")\n\n            except Exception as e:\n                # デコード中にエラーが発生した場合\n                print(f\"デコードエラー: {str(e)}\")\n                raise\n\n        # --- 動画の表示 ---\n        print(\"動画を表示中...\")\n        display_video(output_path)\n\n    except Exception as e:\n        # 動画生成に失敗した場合\n        print(f\"動画生成に失敗しました: {str(e)}\")\n        raise\n    finally:\n        # 成功・失敗に関わらず、最後にメモリをクリア\n        clear_memory()\n\n\ndef display_video(video_path):\n    \"\"\"\n    生成された動画をJupyter Notebook内で表示する関数\n    \n    引数:\n        video_path: 表示する動画ファイルのパス\n    \"\"\"\n    from IPython.display import HTML\n    from base64 import b64encode\n\n    # 動画ファイルをバイナリモードで読み込み\n    mp4 = open(video_path,'rb').read()\n    # Base64エンコードしてHTMLに埋め込める形式に変換\n    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n\n    # HTMLのvideoタグで動画を表示（自動再生、ループ、コントロール付き）\n    display(HTML(f\"\"\"\n    <video width=512 controls autoplay loop>\n        <source src=\"{data_url}\" type=\"video/mp4\">\n    </video>\n    \"\"\"))\n\n# セットアップ完了メッセージ\nprint(\"✅ 環境のセットアップが完了しました！\")"
  },
  {
   "cell_type": "markdown",
   "source": "## ステップ1: 環境の準備\n\nこのセルでは、以下の作業を行います:\n\n### インストール\n1. **PyTorch 2.6.0**: 深層学習フレームワーク\n2. **Diffusers**: 拡散モデルライブラリ\n3. **Accelerate**: モデルの高速化\n4. **ComfyUI**: 動画生成のバックエンド\n5. **av**: 動画ファイル処理\n\n### モデルのダウンロード\n- **ltx-video-2b-v0.9.5.safetensors**: メインの動画生成モデル（約2GBのパラメータ）\n- **t5xxl_fp8_e4m3fn_scaled.safetensors**: テキストエンコーダー（軽量版、8bit量子化）\n\n### 関数の定義\n- `generate_video()`: テキストプロンプトから動画を生成\n- `display_video()`: 生成された動画をノートブック内で表示\n- `clear_memory()`: GPU/CPUメモリを解放\n\n実行には5〜10分かかる場合があります。「Environment Setup Complete!」と表示されれば準備完了です。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ステップ2: 動画生成パラメータの設定と実行\n\n### 調整可能なパラメータ\n\n#### プロンプト設定\n- **positive_prompt**: 生成したい動画の内容を詳細に記述\n  - **重要**: できるだけ詳細で具体的に書く\n  - 例: \"A drone quickly rises through a bank of morning fog, revealing a pristine alpine lake...\"\n  - カメラワーク、被写体、照明、雰囲気などを含めると良い\n- **negative_prompt**: 避けたい要素を指定\n  - 低品質、歪み、動きのブレ、解剖学的な誤りなど\n\n#### 解像度設定\n- **width**: 動画の幅（32の倍数である必要があります）\n- **height**: 動画の高さ（32の倍数である必要があります）\n- **推奨設定**:\n  - 832x480（ワイドスクリーン、デフォルト）\n  - 768x512（バランス型）\n  - 512x768（縦長）\n\n#### 生成パラメータ\n- **seed**: ランダムシード\n  - 0 = ランダム\n  - 特定の値 = 同じ結果を再現可能\n- **steps**: 生成ステップ数（15〜30を推奨）\n  - 多いほど高品質だが時間がかかる\n- **cfg_scale**: プロンプトへの忠実度（1.5〜3.0を推奨）\n  - 大きいほどプロンプトに忠実\n- **sampler_name**: サンプリング方法\n  - **res_multistep**（推奨、デフォルト）\n  - euler, dpmpp_2m, ddim, lms なども選択可能\n\n#### 動画設定\n- **frames**: 生成するフレーム数（n+1フレームを設定してn秒の動画を生成）\n  - 例: 73フレーム = 約3秒（24fps）\n  - 注意: 多すぎるとメモリ不足になる可能性\n- **fps**: フレームレート（通常は24fps）\n\n### プロンプトのコツ\n1. **カメラワークを明示**: \"drone rises\", \"camera glides forward\", \"tracking shot\"\n2. **照明を記述**: \"golden light of sunrise\", \"soft moonlight\", \"harsh shadows\"\n3. **動きを具体的に**: \"slowly moving\", \"quickly rises\", \"gently drifting\"\n4. **雰囲気を追加**: \"dreamy atmosphere\", \"tranquil\", \"dramatic\"\n5. **詳細を含める**: 色、質感、天候、時間帯など\n\n### 実行方法\n1. パラメータを調整\n2. セルを実行\n3. 動画生成が開始されます（数分かかります）\n4. 完了すると、動画が自動的に表示されます\n\n**ヒント**: 最初は小さいフレーム数（25〜49）と低解像度で試して、結果を確認してから調整することをお勧めします。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "roC59_oNNflb"
   },
   "outputs": [],
   "source": "# @title 動画生成パラメータ\n\n# --- プロンプト設定 ---\n# 生成したい動画の内容を詳細に記述（英語推奨）\n# カメラワーク、被写体、照明、雰囲気などを具体的に記述すると良い結果が得られます\npositive_prompt = \"A drone quickly rises through a bank of morning fog, revealing a pristine alpine lake surrounded by snow-capped mountains. The camera glides forward over the glassy water, capturing perfect reflections of the peaks. As it continues, the perspective shifts to reveal a lone wooden cabin with a curl of smoke from its chimney, nestled among tall pines at the lake's edge. The final shot tracks upward rapidly, transitioning from intimate to epic as the full mountain range comes into view, bathed in the golden light of sunrise breaking through scattered clouds.\" # @param {\"type\":\"string\"}\n\n# 避けたい要素を記述（品質低下、歪み、不自然な動きなど）\nnegative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\" # @param {\"type\":\"string\"}\n\n# --- 解像度設定 ---\n# 動画の幅（ピクセル、32の倍数である必要があります）\n# 推奨値: 768, 832, 512など\nwidth = 832 # @param {\"type\":\"number\"}\n\n# 動画の高さ（ピクセル、32の倍数である必要があります）\n# 推奨値: 480, 512, 768など\nheight = 480 # @param {\"type\":\"number\"}\n\n# --- 生成パラメータ ---\n# ランダムシード（0の場合はランダム、それ以外は再現可能な結果）\nseed = 0 # @param {\"type\":\"integer\"}\n\n# 生成ステップ数（15〜30を推奨）\n# 多いほど高品質ですが、時間がかかります\nsteps = 25 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n\n# CFGスケール（Classifier-Free Guidanceのスケール）\n# プロンプトへの忠実度を制御（1.5〜3.0を推奨）\n# 大きいほどプロンプトに忠実になりますが、過度に大きいと不自然になります\ncfg_scale = 2.05 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n\n# サンプリング手法の選択\n# res_multistep: 高品質（推奨）\n# euler: 標準的\n# dpmpp_2m: 高速\n# ddim, lms: その他の手法\nsampler_name = \"res_multistep\" # @param [\"res_multistep\", \"euler\", \"dpmpp_2m\", \"ddim\", \"lms\"]\n\n# --- 動画設定 ---\n# 生成するフレーム数\n# 注意: n秒の動画を作りたい場合は n+1 に設定してください\n# 例: 73フレーム = 約3秒（24fps）、49フレーム = 約2秒\n# フレーム数が多いほどメモリを消費します\nframes = 73 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n\n# フレームレート（1秒あたりのフレーム数）\n# 24fps: 映画品質（推奨）\n# 30fps: 標準的な動画\nfps = 24 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n\n# --- 動画生成の実行 ---\n# 推論モード（学習しない）で実行\nwith torch.inference_mode():\n    # 設定したパラメータで動画を生成\n    generate_video(\n        positive_prompt=positive_prompt,  # ポジティブプロンプト\n        negative_prompt=negative_prompt,  # ネガティブプロンプト\n        width=width,  # 動画の幅\n        height=height,  # 動画の高さ\n        seed=seed,  # ランダムシード\n        steps=steps,  # 生成ステップ数\n        cfg_scale=cfg_scale,  # CFGスケール\n        sampler_name=sampler_name,  # サンプリング手法\n        length=frames,  # フレーム数\n        fps=fps  # フレームレート\n    )\n\n# 生成完了後、メモリをクリア\nclear_memory()"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}