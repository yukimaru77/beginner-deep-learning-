{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 深層学習体験会: 画像から動画生成（開始・終了フレーム指定）\n\nこのノートブックでは、**LTX-Video**モデルを使って、2枚の画像から滑らかな動画を生成する方法を学びます。",
   "metadata": {
    "id": "f4p1ysFKMbs_"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## LTX-Videoとは？\n\n**LTX-Video**は、画像とテキストプロンプトから動画を生成する最新のAIモデルです。このノートブックでは、特に**2枚の画像間を補間する**機能を使用します。\n\n### 主な機能\n- **開始・終了フレーム指定**: 2枚の画像の間を滑らかに補間\n- **プロンプト制御**: テキストで動画の内容を指示\n- **高品質出力**: 滑らかで自然な動画生成\n\n### 使用例\n- 静止画のアニメーション化\n- 商品画像の動的プレゼンテーション\n- キャラクターのポーズ変化アニメーション\n- 風景写真の時間変化表現\n\n## 重要な注意事項\n- 無料のT4 GPUで実行可能ですが、より高速な生成には上位GPUを推奨\n- このノートブックは**シンプルな遷移**（例: 花が成長する、ポーズが変わる）に適しています\n- 激しい動き（歩く、走る）には適していません\n- **詳細なプロンプト**を使用すると、より良い結果が得られます\n\n## このノートブックの流れ\n1. 環境の準備とモデルのダウンロード\n2. 2枚の画像をアップロード\n3. プロンプトとパラメータの設定\n4. 動画の生成と再生",
   "metadata": {
    "id": "EBB00lC6q-DA"
   }
  },
  {
   "cell_type": "code",
   "source": "# --- 環境の準備 ---\n\n# PyTorchとtorchvisionの特定バージョンをインストール（LTX-Video用）\n!pip install torch==2.6.0 torchvision==0.21.0\n\n# /contentディレクトリに移動\n%cd /content\n\n# モデル読み込みの設定\nAlways_Load_Models_for_Inference = False  # 常にモデルを推論用に読み込むか\nUse_t5xxl_fp16 = False  # 16bit精度のテキストエンコーダーを使用するか（Falseで8bit軽量版を使用）\n\n# 必要なライブラリをインストール\n# -q: 静かにインストール（詳細なログを表示しない）\n!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2\n!pip install av  # 動画ファイル処理用\n\n# ComfyUIをクローン（動画生成のバックエンド）\n!git clone https://github.com/Isi-dev/ComfyUI\n%cd /content/ComfyUI\n\n# aria2（高速ダウンローダー）とffmpeg（動画処理）をインストール\n!apt -y install -qq aria2 ffmpeg\n\n# --- モデルファイルのダウンロード ---\n# メインの動画生成モデルをダウンロード\n!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors -d /content/ComfyUI/models/checkpoints -o ltx-video-2b-v0.9.5.safetensors\n\n# テキストエンコーダーのダウンロード（設定に応じて16bitまたは8bit版）\nif Use_t5xxl_fp16:\n    # 16bit版（高精度だが重い）\n    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp16.safetensors\nelse:\n    # 8bit版（軽量で推奨）\n    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp8_e4m3fn_scaled.safetensors\n\n# --- 必要なライブラリをインポート ---\nimport torch  # PyTorch（ディープラーニングフレームワーク）\nimport numpy as np  # 数値計算ライブラリ\nfrom PIL import Image  # 画像処理\nimport gc  # ガベージコレクション（メモリ管理）\nimport sys  # システム関連\nimport random  # 乱数生成\nimport os  # OS操作\nimport imageio  # 動画ファイルの読み書き\nfrom google.colab import files  # Google Colabでのファイルアップロード\nfrom IPython.display import display, HTML  # Jupyter Notebookでの表示\n\n# ComfyUIのパスをシステムパスに追加\nsys.path.insert(0, '/content/ComfyUI')\n\n# ComfyUIのモジュールをインポート\nfrom comfy import model_management\n\n# 基本的なノード（処理単位）をインポート\nfrom nodes import (\n    CheckpointLoaderSimple,  # モデル読み込み\n    CLIPLoader,  # テキストエンコーダー読み込み\n    CLIPTextEncode,  # テキストのエンコード\n    VAEDecode,  # 潜在表現から画像へのデコード\n    LoadImage  # 画像読み込み\n)\n\n# カスタムサンプラー関連のノード\nfrom comfy_extras.nodes_custom_sampler import (\n    KSamplerSelect,  # サンプラーの選択\n    SamplerCustom  # カスタムサンプラー\n)\n\n# LTX-Video専用のノード\nfrom comfy_extras.nodes_lt import (\n    EmptyLTXVLatentVideo,  # 空の潜在動画を作成\n    LTXVPreprocess,  # 画像の前処理\n    LTXVAddGuide,  # ガイド画像の追加\n    LTXVScheduler,  # ノイズスケジュール\n    LTXVConditioning,  # 条件付け\n    LTXVCropGuides  # ガイドのクロップ\n)\n\n# --- ノードのインスタンスを作成 ---\ncheckpoint_loader = CheckpointLoaderSimple()  # モデル読み込み用\nclip_loader = CLIPLoader()  # CLIP（テキストエンコーダー）読み込み用\nclip_encode_positive = CLIPTextEncode()  # ポジティブプロンプトのエンコード用\nclip_encode_negative = CLIPTextEncode()  # ネガティブプロンプトのエンコード用\nload_image = LoadImage()  # 画像読み込み用\nempty_latent = EmptyLTXVLatentVideo()  # 空の潜在動画作成用\npreprocess = LTXVPreprocess()  # 画像前処理用\nadd_guide = LTXVAddGuide()  # ガイド画像追加用\nscheduler = LTXVScheduler()  # スケジューラー用\nsampler_select = KSamplerSelect()  # サンプラー選択用\nconditioning = LTXVConditioning()  # 条件付け用\nsampler = SamplerCustom()  # サンプリング実行用\nvae_decode = VAEDecode()  # デコード用\ncrop_guides = LTXVCropGuides()  # ガイドクロップ用\n\ndef clear_gpu_memory():\n    \"\"\"\n    GPUメモリを解放する関数\n    動画生成後にメモリをクリーンアップするために使用\n    \"\"\"\n    # Pythonのガベージコレクションを実行\n    gc.collect()\n    \n    # CUDAが利用可能な場合\n    if torch.cuda.is_available():\n        # キャッシュをクリア\n        torch.cuda.empty_cache()\n        # プロセス間通信のキャッシュもクリア\n        torch.cuda.ipc_collect()\n    \n    # グローバル変数からテンソルを削除\n    for obj in list(globals().values()):\n        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n            del obj\n    \n    # 再度ガベージコレクション\n    gc.collect()\n\ndef upload_image():\n    \"\"\"\n    Google Colabで画像をアップロードし、ComfyUIのinputディレクトリに保存する関数\n    \n    戻り値:\n        str: 保存された画像のパス（失敗時はNone）\n    \"\"\"\n    from google.colab import files\n    import os\n    import shutil\n\n    # inputディレクトリを作成（既に存在する場合はエラーにしない）\n    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n\n    # ファイルアップロードのUIを表示\n    uploaded = files.upload()\n\n    # アップロードされた各ファイルを処理\n    for filename in uploaded.keys():\n        # 元のパスと保存先のパス\n        src_path = f'/content/ComfyUI/{filename}'\n        dest_path = f'/content/ComfyUI/input/{filename}'\n\n        # ファイルを移動\n        shutil.move(src_path, dest_path)\n        print(f\"画像を保存しました: {dest_path}\")\n        return dest_path\n\n    return None\n\ndef generate_video(\n    image_path: str = None,  # 開始画像のパス\n    guide_image_path: str = None,  # 終了画像のパス\n    positive_prompt: str = \"A red fox moving gracefully, its russet coat vibrant against the white landscape, leaving perfect star-shaped prints behind as steam rises from its breath in the crisp winter air. The scene is wrapped in snow-muffled silence, broken only by the gentle murmur of water still flowing beneath the ice.\",  # ポジティブプロンプト\n    negative_prompt: str = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\",  # ネガティブプロンプト\n    width: int = 768,  # 動画の幅\n    height: int = 512,  # 動画の高さ\n    seed: int = 397166166231987,  # ランダムシード\n    steps: int = 30,  # 生成ステップ数\n    cfg_scale: float = 2.05,  # CFGスケール（プロンプトへの忠実度）\n    sampler_name: str = \"euler\",  # サンプラー名\n    length: int = 97,  # フレーム数\n    fps: int = 24,  # フレームレート\n    guide_strength: float = 0.1,  # ガイド強度\n    guide_frame: int = -1  # ガイドを適用するフレーム位置（-1で最後）\n):\n    \"\"\"\n    2枚の画像から動画を生成するメイン関数\n    \n    引数:\n        image_path: 開始画像のパス\n        guide_image_path: 終了画像のパス\n        positive_prompt: 生成したい内容の説明\n        negative_prompt: 避けたい要素の説明\n        width: 動画の幅（32の倍数）\n        height: 動画の高さ（32の倍数）\n        seed: ランダムシード（再現性のため）\n        steps: 生成ステップ数（多いほど高品質）\n        cfg_scale: プロンプトへの忠実度\n        sampler_name: サンプリング方法\n        length: フレーム数\n        fps: フレームレート\n        guide_strength: 終了画像への影響度\n        guide_frame: 終了画像を配置する位置\n    \"\"\"\n    # 推論モード（学習しない）で実行\n    with torch.inference_mode():\n        print(\"テキストエンコーダーを読み込み中...\")\n        # CLIPテキストエンコーダーを読み込む\n        clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n        print(\"テキストエンコーダーを読み込みました！\")\n\n    try:\n        # --- 解像度の検証 ---\n        # 幅と高さが32の倍数であることを確認\n        assert width % 32 == 0, \"幅は32の倍数である必要があります\"\n        assert height % 32 == 0, \"高さは32の倍数である必要があります\"\n\n        # --- プロンプトのエンコード ---\n        # ポジティブプロンプト（生成したい内容）をエンコード\n        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n        # ネガティブプロンプト（避けたい内容）をエンコード\n        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n\n        # テキストエンコーダーを削除してメモリを解放\n        del clip\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"テキストエンコーダーをメモリから削除しました\")\n\n        # --- 画像のアップロード ---\n        # 開始画像がない場合はアップロードを促す\n        if image_path is None:\n            print(\"開始画像ファイルをアップロードしてください:\")\n            image_path = upload_image()\n        if image_path is None:\n            print(\"開始画像がアップロードされていません！\")\n\n        # 終了画像がない場合はアップロードを促す\n        if guide_image_path is None:\n            print(\"終了画像ファイルをアップロードしてください:\")\n            guide_image_path = upload_image()\n        if guide_image_path is None:\n            print(\"終了画像がアップロードされていません！\")\n\n        # --- 画像の読み込みと前処理 ---\n        # 開始画像を読み込む\n        loaded_image = load_image.load_image(image_path)[0]\n        # 前処理を実行（リサイズなど）\n        processed_image = preprocess.preprocess(loaded_image, 35)[0]\n\n        # 終了画像を読み込む\n        loaded_guide_image = load_image.load_image(guide_image_path)[0]\n        # 前処理を実行\n        processed_guide_image = preprocess.preprocess(loaded_guide_image, 40)[0]\n\n        # --- モデルとVAEの読み込み ---\n        print(\"モデルとVAEを読み込み中...\")\n        # チェックポイントからモデル、CLIP、VAEを読み込む\n        model, _, vae = checkpoint_loader.load_checkpoint(\"ltx-video-2b-v0.9.5.safetensors\")\n        print(\"モデルとVAEを読み込みました！\")\n\n        # --- 空の潜在動画を作成 ---\n        # 指定された幅、高さ、長さで空の潜在表現を作成\n        latent_video = empty_latent.generate(width, height, length)[0]\n\n        # --- 1回目のガイド追加（開始画像） ---\n        # 最初のフレーム（frame_idx=0）に開始画像を追加\n        # strength=1: 完全にガイド画像に従う\n        guided_positive, guided_negative, guided_latent_1 = add_guide.generate(\n            positive=positive,\n            negative=negative,\n            vae=vae,\n            latent=latent_video,\n            image=processed_image,\n            frame_idx=0,  # 最初のフレーム\n            strength=1  # 強度100%\n        )\n\n        # --- 2回目のガイド追加（終了画像） ---\n        # 指定されたフレーム位置に終了画像を追加\n        guided_positive, guided_negative, guided_latent = add_guide.generate(\n            positive=guided_positive,\n            negative=guided_negative,\n            vae=vae,\n            latent=guided_latent_1,\n            image=processed_guide_image,\n            frame_idx=guide_frame,  # 終了フレーム（-1で最後）\n            strength=guide_strength  # ガイド強度（0〜1）\n        )\n\n        # --- サンプリングの準備 ---\n        # ノイズスケジュールを取得（生成プロセスのステップを制御）\n        sigmas = scheduler.get_sigmas(steps, cfg_scale, 0.95, True, 0.1, guided_latent_1)[0]\n        # サンプラーを選択（eulerなど）\n        selected_sampler = sampler_select.get_sampler(sampler_name)[0]\n\n        # --- 条件付けの適用 ---\n        # プロンプトの条件付けを適用\n        conditioned_positive, conditioned_negative = conditioning.append(\n            guided_positive,\n            guided_negative,\n            25.0  # 条件付けの強度\n        )\n\n        print(\"動画を生成中...\")\n\n        # --- 動画のサンプリング ---\n        # メインの生成プロセス\n        sampled = sampler.sample(\n            model=model,  # 使用するモデル\n            add_noise=True,  # ノイズを追加\n            noise_seed=seed if seed != 0 else random.randint(0, 2**32),  # シード\n            cfg=cfg_scale,  # CFGスケール\n            positive=conditioned_positive,  # ポジティブ条件\n            negative=conditioned_negative,  # ネガティブ条件\n            sampler=selected_sampler,  # サンプラー\n            sigmas=sigmas,  # ノイズスケジュール\n            latent_image=guided_latent  # 入力潜在表現\n        )[0]\n\n        # --- ガイドのクロップ ---\n        # ガイドフレームをクロップ（必要に応じて）\n        cropped_latent = crop_guides.crop(\n            conditioned_positive,\n            conditioned_negative,\n            sampled\n        )[2]\n\n        # モデルを削除してメモリを解放\n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"モデルをメモリから削除しました\")\n\n        # --- 潜在表現のデコード ---\n        # 勾配計算なしで実行\n        with torch.no_grad():\n            try:\n                print(\"潜在表現をデコード中...\")\n                # VAEで潜在表現を動画フレームにデコード\n                decoded = vae_decode.decode(vae, cropped_latent)[0].detach()\n                print(\"潜在表現をデコードしました！\")\n                \n                # VAEを削除してメモリを解放\n                del vae\n                torch.cuda.empty_cache()\n                gc.collect()\n                print(\"VAEをメモリから削除しました\")\n            except Exception as e:\n                print(f\"デコード中にエラーが発生しました: {str(e)}\")\n                raise\n\n        # --- MP4ファイルとして保存 ---\n        output_path = \"/content/output.mp4\"\n        # フレームを0-255の範囲のuint8に変換\n        frames_np = (decoded.cpu().numpy() * 255).astype(np.uint8)\n        \n        # imageioで動画ファイルを書き込む\n        with imageio.get_writer(output_path, fps=fps) as writer:\n            for frame in frames_np:\n                writer.append_data(frame)\n\n        # 完了メッセージ\n        print(f\"\\n動画生成が完了しました！\")\n        print(f\"{len(decoded)}フレームを{output_path}に保存しました\")\n        \n        # 動画を表示\n        display_video(output_path)\n\n    except Exception as e:\n        print(f\"動画生成中にエラーが発生しました: {str(e)}\")\n        raise\n    finally:\n        # 最後に必ずメモリをクリーンアップ\n        clear_gpu_memory()\n\ndef display_video(video_path):\n    \"\"\"\n    Colab Notebookで動画を表示する関数\n    HTML5プレーヤーを使用して動画を埋め込み表示\n    \n    引数:\n        video_path: 動画ファイルのパス\n    \"\"\"\n    from IPython.display import HTML\n    from base64 import b64encode\n\n    # 動画ファイルを読み込む\n    mp4 = open(video_path,'rb').read()\n    # Base64エンコードしてdata URLを作成\n    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n\n    # HTML5のvideoタグで表示\n    display(HTML(f\"\"\"\n    <video width=512 controls autoplay loop>\n        <source src=\"{data_url}\" type=\"video/mp4\">\n    </video>\n    \"\"\"))",
   "metadata": {
    "cellView": "form",
    "id": "rrXFIT4fMfyJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ステップ1: 環境の準備\n\nこのセルでは、以下の作業を行います:\n\n### インストール\n1. **PyTorch 2.6.0**: 深層学習フレームワーク\n2. **Diffusers**: 拡散モデルライブラリ\n3. **Accelerate**: モデルの高速化\n4. **ComfyUI**: 動画生成のバックエンド\n\n### モデルのダウンロード\n- **ltx-video-2b-v0.9.5.safetensors**: メインの動画生成モデル\n- **t5xxl_fp8_e4m3fn_scaled.safetensors**: テキストエンコーダー（軽量版）\n\n### 関数の定義\n- `generate_video()`: 動画生成のメイン関数\n- `upload_image()`: 画像アップロード機能\n- `display_video()`: 動画表示機能\n\n実行には5〜10分かかる場合があります。「Environment Setup Complete!」と表示されれば準備完了です。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ステップ2: 動画生成の実行\n\n### 調整可能なパラメータ\n\n#### プロンプト設定\n- **positive_prompt**: 生成したい動画の内容を詳細に記述\n  - 例: \"Flowers growing from the sides of a vase\"（花瓶の側面から花が成長する）\n- **negative_prompt**: 避けたい要素を指定\n  - 例: 低品質、歪み、動きのブレなど\n\n#### 解像度設定\n- **width**: 動画の幅（32の倍数である必要があります）\n- **height**: 動画の高さ（32の倍数である必要があります）\n- 推奨: 512x768 または 768x512\n\n#### 生成パラメータ\n- **seed**: ランダムシード（同じ値で同じ結果を再現）\n- **steps**: 生成ステップ数（多いほど高品質だが時間がかかる）\n- **cfg_scale**: プロンプトへの忠実度（2.0前後を推奨）\n- **sampler_name**: サンプリング方法（euler推奨）\n\n#### 動画設定\n- **frames**: 生成するフレーム数（多いほど長い動画）\n- **guide_strength**: 終了フレームへの影響度（0〜1、1が最大）\n- **guide_frame**: 終了画像を配置するフレーム位置（-1で最後）\n\n### 実行方法\n1. セルを実行すると、最初に**開始画像**をアップロードするよう求められます\n2. 次に**終了画像**をアップロードするよう求められます\n3. 設定したパラメータで動画が生成されます\n4. 完了すると、動画が自動的に表示されます\n\n**ヒント**: 最初は小さいフレーム数（25〜49）で試して、結果を確認してから増やすことをお勧めします。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- 動画生成パラメータの設定 ---\n\n# ポジティブプロンプト: 生成したい動画の内容を詳細に記述\npositive_prompt = \"Flowers growing from the sides of a vase\" # @param {\"type\":\"string\"}\n\n# ネガティブプロンプト: 避けたい要素を列挙\nnegative_prompt = \"low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly\" # @param {\"type\":\"string\"}\n\n# 動画の解像度（32の倍数である必要があります）\nwidth = 512 # @param {\"type\":\"number\"}\nheight = 768 # @param {\"type\":\"number\"}\n\n# ランダムシード: 同じ値で同じ結果を再現できます\nseed = 397166166231987 # @param {\"type\":\"integer\"}\n\n# 生成ステップ数: 多いほど高品質だが時間がかかる\nsteps = 25 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n\n# CFGスケール: プロンプトへの忠実度（2.0前後を推奨）\ncfg_scale = 2.05 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n\n# サンプラー: 生成アルゴリズムの種類\nsampler_name = \"euler\" # @param [\"euler\", \"dpmpp_2m\", \"ddim\", \"lms\"]\n\n# フレーム数: 多いほど長い動画になる\nframes = 49 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n\n# ガイド強度: 終了画像への影響度（0〜1、1が最大）\nguide_strength = 1 # @param {\"type\":\"number\", \"min\":0, \"max\":1}\n\n# ガイドフレーム: 終了画像を配置するフレーム位置（-1で最後のフレーム）\nguide_frame = -1 # @param {\"type\":\"integer\"}\n\n# --- 動画生成の実行 ---\nprint(\"動画生成ワークフローを開始します...\")\n\n# 推論モード（学習しない）で動画生成を実行\nwith torch.inference_mode():\n    generate_video(\n        image_path=None,  # Noneの場合、アップロードを促す\n        guide_image_path=None,  # Noneの場合、アップロードを促す\n        positive_prompt=positive_prompt,\n        negative_prompt=negative_prompt,\n        width=width,\n        height=height,\n        seed=seed,\n        steps=steps,\n        cfg_scale=cfg_scale,\n        sampler_name=sampler_name,\n        length=frames,\n        guide_strength=guide_strength,\n        guide_frame=guide_frame\n    )\n\n# 生成完了後、GPUメモリをクリーンアップ\nclear_gpu_memory()",
   "metadata": {
    "cellView": "form",
    "id": "roC59_oNNflb"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}