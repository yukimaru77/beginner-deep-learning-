{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習体験会: テキスト音声合成（TTS）\n",
    "\n",
    "このノートブックでは、**TTS（Text-to-Speech）**技術を使って、テキストから音声を生成する方法を学びます。\n",
    "\n",
    "## TTSとは？\n",
    "\n",
    "TTS（Text-to-Speech）とは、テキストを読み上げる音声を自動生成する技術です。このノートブックでは、**Zonos**という高品質なTTSモデルを使用します。\n",
    "\n",
    "### Zonosの特徴\n",
    "- 多言語対応（日本語、英語など約100言語）\n",
    "- 感情表現が可能（喜び、悲しみ、怒りなど）\n",
    "- 声のクローニング（参照音声から声質を学習）\n",
    "- ピッチ（音の高さ）と速度の調整が可能\n",
    "\n",
    "### 応用例\n",
    "- オーディオブックの作成\n",
    "- ナビゲーションシステム\n",
    "- アクセシビリティツール（視覚障害者向け）\n",
    "- YouTubeのナレーション\n",
    "- ゲームキャラクターの音声\n",
    "\n",
    "## このノートブックの流れ\n",
    "1. 環境の準備とモデルの読み込み\n",
    "2. 参照音声のアップロード（オプション）\n",
    "3. テキストと感情設定\n",
    "4. 音声の生成と再生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ1: 環境の準備とモデルの読み込み\n",
    "\n",
    "必要なライブラリをインストールし、Zonosモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 環境のセットアップとモデルの読み込み ---\n",
    "\n",
    "# システムの更新とespeak-ngのインストール（音声合成エンジン）\n",
    "!apt update && apt install -y espeak-ng\n",
    "\n",
    "# ZonosのGitHubリポジトリをクローン\n",
    "!git clone https://github.com/Isi-dev/Zonos.git\n",
    "\n",
    "# Zonosディレクトリに移動\n",
    "%cd Zonos\n",
    "\n",
    "# Zonosをインストール（-e: 編集可能モードでインストール）\n",
    "!pip install -e .\n",
    "# !pip install --no-build-isolation -e .[compile] # オプション: ハイブリッドモードで実行する場合に必要\n",
    "\n",
    "# 必要なライブラリをインポート\n",
    "import torch  # PyTorchフレームワーク\n",
    "import torchaudio  # 音声処理用ライブラリ\n",
    "from zonos.model import Zonos  # Zonosモデル\n",
    "from zonos.conditioning import make_cond_dict  # 条件付け用の関数\n",
    "\n",
    "# --- デバイスの確認 ---\n",
    "# CUDAが利用可能ならGPU、そうでなければCPUを使用\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# --- モデルの読み込み ---\n",
    "print(\"モデルを読み込み中...\")\n",
    "# Hugging Faceから事前学習済みモデルをダウンロード\n",
    "model = Zonos.from_pretrained(\"Isi99999/Zonos-v0.1-transformer\", device=device)\n",
    "print(\"モデルの読み込みが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ2: 参照音声のアップロード（オプション）\n",
    "\n",
    "このステップは**オプション**です。参照音声をアップロードすると、その声質を学習してクローンすることができます。\n",
    "\n",
    "### 参照音声の要件\n",
    "- **長さ**: 10秒〜30秒\n",
    "- **形式**: MP3など（自動的に処理されます）\n",
    "- **品質**: クリアで雑音の少ない音声を推奨\n",
    "\n",
    "スキップする場合は、デフォルトの声が使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 参照音声のアップロードと処理 ---\n",
    "\n",
    "# 必要なライブラリをインポート\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# 環境変数を設定（UTF-8エンコーディング）\n",
    "os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n",
    "os.environ[\"LANG\"] = \"C.UTF-8\"\n",
    "\n",
    "# assetsディレクトリを作成（既に存在する場合はエラーにしない）\n",
    "os.makedirs(\"assets\", exist_ok=True)\n",
    "\n",
    "# ファイルアップロードのUIを表示\n",
    "uploaded = files.upload()\n",
    "\n",
    "# アップロードされたファイルを処理\n",
    "for filename in uploaded.keys():\n",
    "    # 保存先のパス\n",
    "    new_path = \"assets/reference.mp3\"\n",
    "    \n",
    "    # 既に同名ファイルが存在する場合は削除\n",
    "    if os.path.exists(new_path):\n",
    "        os.remove(new_path)\n",
    "    \n",
    "    # アップロードされたファイルを安全にリネーム\n",
    "    os.rename(filename, new_path)\n",
    "\n",
    "# --- 参照音声の読み込み ---\n",
    "print(\"参照音声を読み込み中...\")\n",
    "# 音声ファイルを読み込む（wavとサンプリングレートを取得）\n",
    "wav, sampling_rate = torchaudio.load(\"assets/reference.mp3\")\n",
    "\n",
    "# モデルで音声埋め込みベクトルを作成（声の特徴を抽出）\n",
    "speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "print(\"参照音声の読み込みが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ステップ3: テキスト入力と感情設定\n",
    "\n",
    "ここでメインの音声合成を実行します。様々なパラメータを調整できます。\n",
    "\n",
    "### 調整可能なパラメータ\n",
    "\n",
    "1. **text**: 読み上げたいテキスト（日本語にも対応）\n",
    "2. **seed**: ランダムシード（同じ値で同じ結果を再現可能）\n",
    "3. **use_default_speaker**: デフォルトの声を使うか（参照音声を使わない場合はTrue）\n",
    "4. **language**: 言語設定（日本語は'ja'、英語は'en-us'）\n",
    "\n",
    "#### 感情パラメータ（0.0〜1.0の範囲）\n",
    "- **happy**: 喜び\n",
    "- **sad**: 悲しみ\n",
    "- **disgust**: 嫌悪\n",
    "- **fear**: 恐怖\n",
    "- **surprise**: 驚き\n",
    "- **anger**: 怒り\n",
    "- **neutral**: 中立\n",
    "- **other**: その他\n",
    "\n",
    "**注意**: 感情パラメータの合計は自動的に1.0に正規化されます。\n",
    "\n",
    "#### 音声パラメータ\n",
    "- **pitch**: ピッチ（音の高さ）0〜400\n",
    "- **speed**: 話す速度 0〜40（15が標準）\n",
    "\n",
    "### 実行方法\n",
    "1. パラメータを調整\n",
    "2. セルを実行\n",
    "3. 音声が生成され、自動的に再生されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- パラメータ設定 ---\n",
    "# 読み上げるテキスト（英語のサンプル文）\n",
    "text = \" I am motivated by the simple yet profound joys of being alive—the taste of a good meal, the laughter of a friend, the beauty of a sunrise, and the endless pursuit of knowledge. Even if everything about me ceases when I die, my actions, words, and ideas can leave ripples in the world, affecting others in ways I may never fully grasp. \" # @param {type:\"string\"}\n",
    "\n",
    "# ランダムシード（同じ値で同じ結果を再現）\n",
    "seed = 421 # @param {\"type\":\"number\"}\n",
    "\n",
    "# デフォルトの声を使用するかどうか\n",
    "use_default_speaker = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# 言語設定（'en-us': 英語, 'ja': 日本語）\n",
    "language = 'en-us' # @param ['af', 'am', 'an', 'ar', 'as', 'az', 'ba', 'bg', 'bn', 'bpy', 'bs', 'ca', 'cmn', 'cs', 'cy', 'da', 'de', 'el', 'en-029', 'en-gb', 'en-gb-scotland', 'en-gb-x-gbclan', 'en-gb-x-gbcwmd', 'en-gb-x-rp', 'en-us', 'eo', 'es', 'es-419', 'et', 'eu', 'fa', 'fa-latn', 'fi', 'fr-be', 'fr-ch', 'fr-fr', 'ga', 'gd', 'gn', 'grc', 'gu', 'hak', 'hi', 'hr', 'ht', 'hu', 'hy', 'hyw', 'ia', 'id', 'is', 'it', 'ja', 'jbo', 'ka', 'kk', 'kl', 'kn', 'ko', 'kok', 'ku', 'ky', 'la', 'lfn', 'lt', 'lv', 'mi', 'mk', 'ml', 'mr', 'ms', 'mt', 'my', 'nb', 'nci', 'ne', 'nl', 'om', 'or', 'pa', 'pap', 'pl', 'pt', 'pt-br', 'py', 'quc', 'ro', 'ru', 'ru-lv', 'sd', 'shn', 'si', 'sk', 'sl', 'sq', 'sr', 'sv', 'sw', 'ta', 'te', 'tn', 'tr', 'tt', 'ur', 'uz', 'vi', 'vi-vn-x-central', 'vi-vn-x-south', 'yue']\n",
    "\n",
    "# --- 感情パラメータ（0.0〜1.0の範囲） ---\n",
    "happy = 0.3077 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 喜び\n",
    "sad = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 悲しみ\n",
    "disgust = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 嫌悪\n",
    "fear = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 恐怖\n",
    "surprise = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 驚き\n",
    "anger = 0.0256 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 怒り\n",
    "other = 0.2564 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # その他\n",
    "neutral = 0.3077 # @param {type:\"slider\", min:0.0, max:1.0, step:0.05} # 中立\n",
    "\n",
    "# --- 音声パラメータ ---\n",
    "pitch = 20 # @param {type:\"slider\", min:0, max:400, step:1} # ピッチ（音の高さ）\n",
    "speed = 15 # @param {type:\"slider\", min:0.0, max:40.0, step:1.0} # 話す速度\n",
    "\n",
    "# --- 感情パラメータの正規化 ---\n",
    "# 全ての感情値の合計を計算\n",
    "total = happy + sad + disgust + fear + surprise + anger + other + neutral\n",
    "if total > 0:\n",
    "    # 合計が1.0になるように正規化\n",
    "    happy = happy / total\n",
    "    sad = sad / total\n",
    "    disgust = disgust / total\n",
    "    fear = fear / total\n",
    "    surprise = surprise / total\n",
    "    anger = anger / total\n",
    "    other = other / total\n",
    "    neutral = neutral / total\n",
    "\n",
    "# 感情値をテンソルに変換\n",
    "emotions = torch.tensor(list(map(float, [happy, sad, disgust, fear, surprise, anger, other, neutral])), device=device)\n",
    "\n",
    "# --- デフォルトの声を使用する場合 ---\n",
    "if use_default_speaker:\n",
    "    print(\"デフォルトの音声を読み込み中...\")\n",
    "    # デフォルトの音声ファイルを読み込む\n",
    "    wav, sampling_rate = torchaudio.load(\"assets/exampleaudio.mp3\")\n",
    "    # 音声埋め込みベクトルを作成\n",
    "    speaker = model.make_speaker_embedding(wav, sampling_rate)\n",
    "    print(\"デフォルトの音声を読み込みました！\")\n",
    "\n",
    "\n",
    "def generate_speech2(text, seed=421, language=\"en-us\", emotion_tensor=torch.tensor(list(map(float, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])), device=device), pitch=20, speed=15):\n",
    "    \"\"\"\n",
    "    テキストから音声を生成する関数\n",
    "    \n",
    "    引数:\n",
    "        text: 読み上げるテキスト\n",
    "        seed: ランダムシード\n",
    "        language: 言語コード\n",
    "        emotion_tensor: 感情パラメータのテンソル\n",
    "        pitch: ピッチ（音の高さ）\n",
    "        speed: 話す速度\n",
    "    \n",
    "    戻り値:\n",
    "        生成された音声ファイルのパス\n",
    "    \"\"\"\n",
    "    print(f\"音声を生成中: {text}\")\n",
    "\n",
    "    # ランダムシードの設定\n",
    "    if seed >= 0:\n",
    "        torch.manual_seed(seed)  # 再現性のためにシードを固定\n",
    "    else:\n",
    "        torch.random.seed()  # ランダムなシードを使用\n",
    "\n",
    "    # --- 条件付けの作成 ---\n",
    "    # テキスト、言語、声、感情などの条件を辞書にまとめる\n",
    "    cond_dict = make_cond_dict(\n",
    "        text=text,  # 読み上げるテキスト\n",
    "        language=language,  # 言語\n",
    "        speaker=speaker,  # 声の特徴\n",
    "        emotion=emotion_tensor,  # 感情パラメータ\n",
    "        pitch_std=pitch,  # ピッチの標準偏差\n",
    "        speaking_rate=speed  # 話す速度\n",
    "    )\n",
    "    # モデルが理解できる形式に変換\n",
    "    conditioning = model.prepare_conditioning(cond_dict)\n",
    "\n",
    "    # --- 音声の生成 ---\n",
    "    # モデルで音声コードを生成\n",
    "    codes = model.generate(conditioning)\n",
    "    # コードを音声波形にデコード（CPUに転送）\n",
    "    wavs = model.autoencoder.decode(codes).cpu()\n",
    "\n",
    "    # --- 音声の保存 ---\n",
    "    filename = \"output.wav\"\n",
    "    # WAVファイルとして保存\n",
    "    torchaudio.save(filename, wavs[0], model.autoencoder.sampling_rate)\n",
    "    return filename\n",
    "\n",
    "# --- 音声生成の実行 ---\n",
    "output_file = generate_speech2(\n",
    "    text, \n",
    "    seed=seed, \n",
    "    language=language, \n",
    "    emotion_tensor=emotions, \n",
    "    pitch=pitch, \n",
    "    speed=speed\n",
    ")\n",
    "\n",
    "# --- 音声の再生 ---\n",
    "from IPython.display import Audio\n",
    "# Jupyter Notebookで音声を再生\n",
    "Audio(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
